{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10.4_Question_XGBOOST .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAxMjpw8UjaxhlNlNtHoMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebagit914/Challenge01/blob/master/10_4_Question_XGBOOST_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elqBjhcQo8hG",
        "outputId": "0c5d86d2-b854-47de-bac4-f39178232e19"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUgvUV-gsDBc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voU1WfaIvIlx"
      },
      "source": [
        "Reducción de dimensionalidad - Preguntas de entrevista.\n",
        "\n",
        "¿Qué es boosting?\n",
        "\n",
        "¿Cuál es la diferencia entre bagging(ensacado) y boosting(impulso)?\n",
        "\n",
        "¿Cómo difiere la potenciación del gradiente de algoritmos de árboles de decisiones tradicionales?\n",
        "\n",
        "¿Qué hiperparámetros pueden calibrarse en potenciación del gradiente, además de cada hiperparámetro individual de los árboles?\n",
        "\n",
        "¿Cómo puedes reducir el sobreajuste al potenciar la gradiente?\n",
        "\n",
        "¿Por qué múltiples árboles débiles al azar podrían ser mejor que un solo árbol largo? \n",
        "\n",
        "¿Qué es potenciación del gradiente?\n",
        "\n",
        "¿Qué es stacking?\n",
        "\n",
        "¿Cuáles son los pros y contras del bagging?\n",
        "\n",
        "¿Cuáles son los pros y contras del boosting?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybblz7ikvJAl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IrTQK04vcvt"
      },
      "source": [
        "Boosting - Interview Question Solutions\n",
        "\n",
        "1. What is boosting?\n",
        "    - Boosting is an ensemble method that builds models sequentially,\n",
        "      at each step emphasizing the observations that were missed in previous steps.\n",
        "      This reduces bias.\n",
        "\n",
        "2. What is the difference between bagging and boosting?\n",
        "    - Bagging and boosting are both ensemble methods, meaning they combine many\n",
        "      weak predictors to create a strong predictor. One key difference is that\n",
        "      bagging builds independent models in parallel, whereas boosting builds models\n",
        "      sequentially, at each step emphasizing the observations that were missed in\n",
        "      previous steps.\n",
        "\n",
        "3. How does gradient boosting differ from traditional decision tree algorithms?\n",
        "    - Gradient boosting involves using multiple weak predictors (decision trees)\n",
        "      to create a strong predictor. Specifically, it includes a loss function that\n",
        "      calculates the gradient of the error with regard to each feature and then\n",
        "      iteratively creates new decision trees that minimize the current error.\n",
        "      More and more trees are added to the current model to continue correcting\n",
        "      error until improvements fall below some minimum threshold or a pre-decided\n",
        "      number of trees have been created.\n",
        "\n",
        "4. What hyperparameters can be tuned in gradient boosting that are in addition\n",
        "   to each individual tree's hyperparameters?\n",
        "    - The main hyperparameters that can be tuned with GBM models are:\n",
        "        - Loss function - loss function to calculate gradient of error\n",
        "        - Learning rate - the rate at which new trees correct/modify the existing predictor\n",
        "        - Num estimators - the total number of tress to produce for the final predictor\n",
        "        - Additional hyperparameters specific to the loss function\n",
        "\n",
        "5. How can you reduce overfitting when doing gradient boosting?\n",
        "    - Reducing the learning rate or reducing the maximum number of estimators are\n",
        "      the two easiest ways to deal with gradient boosting models that overfit the data.\n",
        "      With stochastic gradient boosting, reducing subsample size is an additional\n",
        "      way to combat overfitting. Boosting algorithms tend to be vulnerable to\n",
        "      overfitting, so knowing how to reduce overfitting is important.\n",
        "\n",
        "6. Why might multiple random weak trees be better than one long tree?\n",
        "    - One long tree is going to have very high variance, where multiple random\n",
        "      weak trees will not be as overfit.\n",
        "\n",
        "7. What is gradient boosting?\n",
        "    - Gradient boosting is a boosting model that works by sequentially fitting\n",
        "      models on the errors of the previous models. It is a combination of\n",
        "      several weak learners, typically decision trees.\n",
        "\n",
        "8. What is stacking?\n",
        "    - Stacking is an ensemble method that fits multiple models on the outputs or\n",
        "      predictions of previous models. The final model is “stacked” on top of others.\n",
        "\n",
        "9. What are the pros and cons of bagging?\n",
        "    - Pros: Reduces variance\n",
        "    - Cons: Not helpful if you have high bias\n",
        "\n",
        "10. What are the pros and cons of boosting?\n",
        "    - Pros: Reduces bias\n",
        "    - Cons: Can increase variance, Increases complexity, Can be time &\n",
        "      computationally expensive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H-MjiGEvdMN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}