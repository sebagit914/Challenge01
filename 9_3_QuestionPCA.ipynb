{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9.3.QuestionPCA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrXJDiwSqXFAbkGO4SHc4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebagit914/Challenge01/blob/master/9_3_QuestionPCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjXTbdEebMRe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7_6gZwhNC9h"
      },
      "source": [
        "Reducción de dimensionalidad - Preguntas de entrevista.\n",
        "\n",
        "¿Por qué querrías usar técnicas de reducción de dimensionalidad para transformar tus datos antes de entrenarlos?\n",
        "\n",
        "¿Por qué querrías evitar técnicas de reducción de dimensionalidad para transformar tus datos antes de entrenarlos?\n",
        "\n",
        "Nombra un algoritmo popular de reducción de dimensionalidad y descríbelo de forma breve.\n",
        "\n",
        "Después de reducir la dimensionalidad, ¿puedes transformar los datos al espacio de características original? ¿Cómo?\n",
        "\n",
        "¿Cómo seleccionas el número de componentes principales necesarios para el ACP? \n",
        "¿Cuándo usarías el ACP?   \n",
        "\n",
        "¿Por qué llevarías a cabo el ACP incluso sin tener una gran cantidad de características?\n",
        "\n",
        "¿En qué casos NO llevarías a cabo un ACP?  \n",
        "\n",
        "¿Cuál es la interpretación geométrica de un vector propio y un valor propio?\n",
        "\n",
        "¿Cuál es la interpretación algebraica de un vector propio y un valor propio?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmUUhbxyOLqw"
      },
      "source": [
        "Dimensionality Reduction - Interview Question Solutions\n",
        "\n",
        "1. Why would you want to use dimensionality reduction techniques to transform\n",
        "   your data before training?\n",
        "    - Dimensionality reduction can allow you to:\n",
        "        - Remove collinearity from the feature space\n",
        "        - Speed up training by reducing the number of features\n",
        "        - Reduce memory usage by reducing the number of features\n",
        "        - Identify underlying, latent features that impact multiple features in\n",
        "          the original space\n",
        "\n",
        "2. Why would you want to avoid dimensionality reduction techniques to transform\n",
        "   your data before training?\n",
        "    - Dimensionality reduction can:\n",
        "        - Add extra unnecessary computation\n",
        "        - Make the model difficult to interpret if the latent features are not\n",
        "          easy to understand\n",
        "        - Add complexity to the model pipeline\n",
        "        - Reduce the predictive power of the model if too much signal is lost\n",
        "\n",
        "3. Name a popular dimensionality reduction algorithm and briefly describe it.\n",
        "    - Principal component analysis (PCA) - uses an eigen decomposition to\n",
        "      transform the original feature data into linearly independent eigenvectors.\n",
        "      The most important vectors (with highest eigenvalues) are then selected to\n",
        "      represent the features in the transformed space\n",
        "\n",
        "4. After doing dimensionality reduction, can you transform the data back into\n",
        "   the original feature space? How?\n",
        "    - Yes and no. Most dimensionality reduction techniques have inverse\n",
        "      transformations, but signal is often lost when reducing dimensions,\n",
        "      so the inverse transformation is usually only an approximation of the original data.\n",
        "\n",
        "5. How do you select the number of principal components needed for PCA?\n",
        "    - Selecting the number of latent features to retain is typically done by\n",
        "      inspecting the eigenvalue of each eigenvector. As eigenvalues decrease,\n",
        "      the impact of the latent feature on the target variable also decreases.\n",
        "      This means that principal components with small eigenvalues have a small\n",
        "      impact on the model and can be removed. There are various rules of thumb,\n",
        "      but one general rule is to include the most significant principal components\n",
        "      that account for at least 95% of the variation in the features.\n",
        "\n",
        "6. When would you use PCA?\n",
        "    - PCA is great for dimensionality reduction. It can be used for visualizing\n",
        "      high-dimensional data or speeding up machine learning algorithms.\n",
        "\n",
        "7. Why would you perform PCA even if you don't have a lot of features?\n",
        "    - You can visualize data that is more than 3 dimensional using PCA.\n",
        "\n",
        "8. In what cases would you NOT use PCA?\n",
        "    - If you are mostly concerned with interpretability or keeping all of the features,\n",
        "      PCA might not be the best choice.\n",
        "\n",
        "9. What is the geometric interpretation of an eigenvector and eigenvalue?\n",
        "    - An eigenvector points in the direction of transformation. The eigenvalue is\n",
        "      the amount by which it is stretched.\n",
        "\n",
        "10. What is the algebraic interpretation of an eigenvector and eigenvalue?\n",
        "    - An eigenvector is a vector that changes by a scalar factor when that linear\n",
        "      transformation is applied to it. The eigenvalue denotes the factor by which\n",
        "      the eigenvector is scaled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJysCAhYOMdE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}